{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* basic\n",
    "    * 가장 기초적 전처리\n",
    "    * punctuation 제거\n",
    "* spell check\n",
    "    * 오탈자 교정\n",
    "    * 줄임말 원형 복원 (i'm not happy -> I am not happy)\n",
    "* part of speech\n",
    "    * 형태소분석\n",
    "    * noun, adjective, verb, adverb\n",
    "* stemming\n",
    "    * 형태소 분석 후 동사 원형 복원\n",
    "* stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv(\"data/review_datal_all.csv\")\n",
    "review = pd.DataFrame(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6653"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = list(review.text)\n",
    "len(test) #15개의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#1.띄워쓰기\\nfrom pykospacing import spacing  \\nspace_doc=[]\\nfor doc in test:\\n    space_doc.append(spacing(doc))\\n#2. 맞춤법과 한글만 남기기\\nfrom hanspell import spell_checker\\nchecked_doc =[]\\nfor doc in test:\\n    check = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc) #한글\\n    check = spell_checker.check(check).checked #맞춤법\\n    checked_doc.append(check) \\n    \\n#3.stopword\\npath = \\'data/stopwords-ko.txt\\'\\nwith open(path,encoding=\"utf-8\") as f:\\n    stopwords =[word for word in f]\\n\\n#4.tokenization - 품사 (형용사, 동사에 감성분석)\\ntokenized_doc=[]\\nword_doc = []\\ntokenization = input()\\nfrom konlpy.tag import Okt\\ntokenizer = Okt()\\nfor doc in checked_doc:\\n    if(tokenization == \"품사\"):\\n        token = [pair for pair in tokenizer.pos(doc) if pair[0] not in stopwords and len(pair[0]) > 1] \\n        words = [word for word, pos in token]\\n        tokenized_doc.append(words)\\n        word_doc.append(token)\\n    if(tokenization == \"단어\"):\\n        token = [word for word in doc.split() if word not in stopwords and len(word) > 1] \\n        tokenized_doc.append(token)\\n    if(tokenization == \"형태소\"):\\n        token = [word for word in tokenizer.morphs(doc,stem=True) if word not in stopwords and len(word) > 1] \\n        tokenized_doc.append(token)\\nprint(len(tokenized_doc))\\nprint(len(word_doc)) #품사일때\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#제일 처음에 사용하던 전처리\n",
    "\"\"\"\n",
    "#1.띄워쓰기\n",
    "from pykospacing import spacing  \n",
    "space_doc=[]\n",
    "for doc in test:\n",
    "    space_doc.append(spacing(doc))\n",
    "#2. 맞춤법과 한글만 남기기\n",
    "from hanspell import spell_checker\n",
    "checked_doc =[]\n",
    "for doc in test:\n",
    "    check = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc) #한글\n",
    "    check = spell_checker.check(check).checked #맞춤법\n",
    "    checked_doc.append(check) \n",
    "    \n",
    "#3.stopword\n",
    "path = 'data/stopwords-ko.txt'\n",
    "with open(path,encoding=\"utf-8\") as f:\n",
    "    stopwords =[word for word in f]\n",
    "\n",
    "#4.tokenization - 품사 (형용사, 동사에 감성분석)\n",
    "tokenized_doc=[]\n",
    "word_doc = []\n",
    "tokenization = input()\n",
    "from konlpy.tag import Okt\n",
    "tokenizer = Okt()\n",
    "for doc in checked_doc:\n",
    "    if(tokenization == \"품사\"):\n",
    "        token = [pair for pair in tokenizer.pos(doc) if pair[0] not in stopwords and len(pair[0]) > 1] \n",
    "        words = [word for word, pos in token]\n",
    "        tokenized_doc.append(words)\n",
    "        word_doc.append(token)\n",
    "    if(tokenization == \"단어\"):\n",
    "        token = [word for word in doc.split() if word not in stopwords and len(word) > 1] \n",
    "        tokenized_doc.append(token)\n",
    "    if(tokenization == \"형태소\"):\n",
    "        token = [word for word in tokenizer.morphs(doc,stem=True) if word not in stopwords and len(word) > 1] \n",
    "        tokenized_doc.append(token)\n",
    "print(len(tokenized_doc))\n",
    "print(len(word_doc)) #품사일때\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic and check spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. korean\n",
    "import re\n",
    "clean_doc =[]\n",
    "for doc in test:\n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc) #한글\n",
    "    clean_doc.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soynlp in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (0.0.493)\n",
      "Requirement already satisfied: psutil>=5.0.1 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from soynlp) (5.7.2)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from soynlp) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from soynlp) (0.23.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from soynlp) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.20.0->soynlp) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.20.0->soynlp) (2.1.0)\n",
      "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /private/var/folders/0y/sw67dqxs30d9l05l8m7j8kfm0000gn/T/pip-req-build-kodk6rbx\n",
      "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /private/var/folders/0y/sw67dqxs30d9l05l8m7j8kfm0000gn/T/pip-req-build-kodk6rbx\n",
      "Requirement already satisfied: tensorflow==2.4.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from pykospacing==0.4) (2.4.0)\n",
      "Requirement already satisfied: keras>=2.4.3 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from pykospacing==0.4) (2.4.3)\n",
      "Requirement already satisfied: h5py==2.10.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from pykospacing==0.4) (2.10.0)\n",
      "Collecting argparse>=1.4.0\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from h5py==2.10.0->pykospacing==0.4) (1.19.2)\n",
      "Requirement already satisfied: six in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from h5py==2.10.0->pykospacing==0.4) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.14.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.32.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.11.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.35.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from keras>=2.4.3->pykospacing==0.4) (1.5.2)\n",
      "Requirement already satisfied: pyyaml in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from keras>=2.4.3->pykospacing==0.4) (5.3.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.26.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (50.3.1.post20201107)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.25.11)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.1.0)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-hanspell.git\n",
      "  Cloning https://github.com/ssut/py-hanspell.git to /private/var/folders/0y/sw67dqxs30d9l05l8m7j8kfm0000gn/T/pip-req-build-ii3c74yz\n",
      "  Running command git clone -q https://github.com/ssut/py-hanspell.git /private/var/folders/0y/sw67dqxs30d9l05l8m7j8kfm0000gn/T/pip-req-build-ii3c74yz\n",
      "Requirement already satisfied: requests in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from py-hanspell==1.1) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests->py-hanspell==1.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests->py-hanspell==1.1) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/saeran/opt/anaconda3/lib/python3.8/site-packages (from requests->py-hanspell==1.1) (2.10)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   408    0   408    0     0    438      0 --:--:-- --:--:-- --:--:--   437\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 19779  100 19779    0     0   9954      0  0:00:01  0:00:01 --:--:--  9954\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "normalize-soynlp\n",
    "띄워쓰기 pykospacing\n",
    "맞춤법 hanspell\n",
    "외래어 사전 다운로드\n",
    "'''\n",
    "!pip install soynlp \n",
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "!pip install git+https://github.com/ssut/py-hanspell.git\n",
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" -o confused_loanwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lownword_map = {}\n",
    "lownword_data = open('confused_loanwords.txt', 'r', encoding='utf-8')\n",
    "\n",
    "lines = lownword_data.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    miss_spell = line.split('\\t')[0]\n",
    "    ori_word = line.split('\\t')[1]\n",
    "    lownword_map[miss_spell] = ori_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.19.2\n",
      "  Downloading numpy-1.19.2-cp38-cp38-macosx_10_9_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 415 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.0\n",
      "    Uninstalling numpy-1.19.0:\n",
      "      Successfully uninstalled numpy-1.19.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import spacing\n",
    "from hanspell import spell_checker\n",
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_preprocessed_doc = []\n",
    "for doc in clean_doc:\n",
    "    spaced_doc = spacing(doc) #띄워쓰기\n",
    "    spelled_doc = spell_checker.check(spaced_doc) #맞춤법\n",
    "    checked_doc = spelled_doc.checked\n",
    "    normalized_doc = repeat_normalize(checked_doc) #정규화\n",
    "    for lownword in lownword_map:\n",
    "        normalized_doc = normalized_doc.replace(lownword, lownword_map[lownword])\n",
    "    spell_preprocessed_doc.append(normalized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "review[\"spell_preprocessed\"] = spell_preprocessed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizatioin and stemming\n",
    "- mecab (stemming까지 추가해서 비교해볼 예정)\n",
    "- khaiii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mecab \n",
    "#3.stopword\n",
    "path = 'data/stopwords-ko.txt'\n",
    "with open(path,encoding=\"utf-8\") as f:\n",
    "    stopwords =[word for word in f]\n",
    "\n",
    "#4.tokenization - 품사 (형용사, 동사에 감성분석)\n",
    "tokenized_doc=[]\n",
    "word_doc = []\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "for doc in spell_preprocessed_doc:\n",
    "    token = [pair for pair in tokenizer.pos(doc) if pair[0] not in stopwords and len(pair[0]) > 1] \n",
    "    words = [word for word, pos in token]\n",
    "    tokenized_doc.append(words)\n",
    "    word_doc.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#khaiii\n",
    "from khaiii import KhaiiiApi\n",
    "api = KhaiiiApi()\n",
    "\n",
    "significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
    "\n",
    "def pos_text(texts):\n",
    "    corpus = []\n",
    "    error = []\n",
    "    for sent in texts:\n",
    "        pos_tagged = ''\n",
    "        try:\n",
    "            words = api.analyze(sent)\n",
    "            for word in words:\n",
    "                for morph in word.morphs:\n",
    "                    if morph.tag in significant_tags:\n",
    "                        pos_tagged += morph.lex + '/' + morph.tag + ' '\n",
    "            corpus.append(pos_tagged.strip())\n",
    "        except:\n",
    "                error.append(sent)\n",
    "                corpus.append(\"\")\n",
    "    return corpus, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6653 102\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_corpus, error = pos_text(spell_preprocessed_doc)\n",
    "print(len(pos_tagged_corpus),len(error)) #error는 수작업..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동사를 원형으로 복원하도록 하겠습니다.\n",
    "규칙은 다음과 같습니다.\n",
    "\n",
    "1. NNG|NNP|NNB + XSV|XSA --> NNG|NNP|NNB + XSV|XSA + 다\n",
    "2. NNG|NNP|NNB + XSA + VX --> NNG|NNP + XSA + 다\n",
    "3. VV --> VV + 다\n",
    "4. VX --> VX + 다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n",
    "p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n",
    "p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n",
    "p4 = re.compile('[가-힣A-Za-z0-9]+/VX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        ori_sent = sent\n",
    "        mached_terms = re.findall(p1, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        mached_terms = re.findall(p2, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                if tag != 'VX':\n",
    "                    modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p3, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p4, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        corpus.append(ori_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_corpus = stemming_text(pos_tagged_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "판례/NNG 평석/NNG 잘/MAG 하/XSV 성적/NNG 잘/MAG 주다/VV 것/NNB 같/VA 중간/NNG 개/NNG 망치다/VV 기말/NNG 틀리다/VV 문제/NNG 개/NNB 있다/VV 판례/NNG 평석/NNG 과제/NNG 평가/NNG 전부/NNG 성적/NNG 결국/NNG 나오다/VV\n",
      "과제/NNG 되다/VV 귀찮/VA 많/VA 시험/NNG 말발/NNG 좋/VA 되다/VV 시험/NNG 망치다/VV 과제/NNG 열심히/MAG 하다/VV 점수/NNG 잘/MAG 받다/VV\n",
      "수업/NNG 띄우다/VV 놓다/VV 하다/VV 보통/NNG 교안/NNG 없/VA 관련/NNG 사례/NNG 중심/NNG 많이/MAG 얘기하다/VV 심/NNG 수업/NNG 잘/MAG 듣다/VV 시험/NNG 잘/MAG 보다/VV 수/NNB 있다/VV 성적/NNG 후하다/VV 주다/VV 편/NNB 다만/MAG 과제/NNG 분량/NNG 채우다/VV 쓰다/VV 쉽/VA 않다/VV\n",
      "교재/NNG 없/VA 내용/NNG 많이/MAG 얘기하다/VV 주다/VV 말씀하다/VV 내용/NNG 시험/NNG 문제/NNG 내다/VV 교재/NNG 그렇/VA 크/VA 필요/NNG 없/VA 과제/NNG 좀/MAG 시간/NNG 오래/MAG 걸리다/VV 하다/VV 데/NNG 판례/NNG 평석/NNG 민사/NNG 소송법/NNG 강의/NNG 내주다/VV 어차피/MAG 내년/NNG 하다/VV 되다/VV 계속하다/VV 되다/VV 거/NNB 연습하다/VV 다/MAG 셈/NNB 치다/VV 해/NNG 보다/VV 길/NNG 점수/NNG 나/NNB 름/NNG 잘/MAG 주다/VV 것/NNB 같/VA 방청/NNG 재미있/VA 상당히/MAG 만족하다/VV 강의/NNG 강의/NNG 내용/NNG 재밌/VA 시험/NNG 문제/NNG 괜찮/VA 학점/NNG 원래/NNG 잘/MAG 주다/VV 교수/NNG 법학/NNG 과/NNG 들어오다/VV 강의/NNG 듣다/VV 전/NNG 수강하다/VV 최적되다/VV 과목/NNG 듯/NNB\n",
      "사이버/NNG 강의라/NNG 괜찮/VA 과제/NNG 꽤/MAG 많/VA 시험/NNG 수업/NNG 잘/MAG 듣다/VV 잘/MAG 나오다/VV\n",
      "과제/NNG 힘/VA 듦/NNG 수업/NNG 집중하다/VV 재미/NNG 있다/VV 매우/MAG 졸다/VV 림/NNG\n",
      "교수/NNG 진짜/NNG 좋/VA 재밌/VA 수업하다/VV 노력하다/VV 이름/NNG 외/NNB 우다/VV 노력하다/VV 것/NNB 눈/NNG 보이다/VV 다만/MAG 재미/NNG 없/VA 시/NNG 종/NNG 치다/VV 때쯤/NNG 되다/VV 앞/NNG 애/NNG 다/MAG 졸다/VV 있다/VV 평석/NNG 방청/NNG 같/VA 과/NNG 있다/VV 글/NNG 쓰다/VV 양/NNG 늘리다/VV 잘/MAG 쓰다/VV 사람/NNG 문제없/VA 성/NNG 만/NNB 있다/VV 이쁘/VA 주다/VV 것/NNB 같/VA 수업/NNG 시간/NNG 말하다/VV 사례/NNG 위주/NNG 공부하다/VV 성적/NNG 나쁘/VA 않다/VV 나오다/VV\n",
      "교재/NNG 정말/MAG 필요/NNG 없/VA 당연히/MAG 시험/NNG 문제/NNG 그다지/MAG 영향/NNG 안/MAG 줌/NNG 교수/NNG 수업/NNG 전/NNG 따로/MAG 호명하다/VV 눈/NNG 마주치다/VV 심/NNG 수업/NNG 내용/NNG 좋/VA 과제/NNG 많이/MAG 빡/MAG 셈/NNG 내용/NNG 빡/MAG 세/NNG 분량/NNG 채우다/VV 힘들/VA 것/NNB 큼/NNB\n",
      "일단/MAG 법학부/NNG 수업/NNG 중/NNB 재미있/VA 그리고/MAJ 열공하다/VV 성적/NNG 또한/MAG 뒤/NNG 따/NNB 르/VA 분/MAG 하/XSA\n",
      "교재/NNG 꼼꼼히/MAG 안/MAG 읽다/VV 교수/NNG 강/NNG 녹음하다/VV 놓다/VV 시험/NNG 기간/NNG 전/NNG 번/NNB 듣다/VV 보다/VV 하다/VV 시험/NNG 어렵/VA 않다/VV 풀다/VV 수/NNB 있다/VV 문제/NNG 대부분/NNG 직접/MAG 평결/NNG 내리다/VV 형식임/NNG 학기/NNG 과제/NNG 번/NNB 다/MAG 장/NNG 최소/NNG 분량/NNG 그런데/MAJ 분량/NNG 안/MAG 채우다/VV 내용/NNG 애쓰다/VV 티/NNG 나다/VV 점수/NNG 잘/MAG 주다/VV 피드백/NNG 있다/VV 좋/VA\n",
      "제대로/MAG 들다/VV 보다/VV 배우다/VV 것/NNB 많/VA 재밌/VA 강의/NNG 그러나/MAJ 제대로/MAG 듣다/VV 힘들/VA 강/NNG 매우/MAG 졸림/NNG 과제/NNG 번/NNB 있다/VV 과제하다/VV 하다/VV 수/NNB 있다/VV 과제/NNG 아/NNG 글/NNG 잘/MAG 쓰다/VV 거/NNB 좋아다/VV 하/XSA 그러다/VV 열심히/MAG 하다/VV 이상/NNG 주/NNG 심/NNG 시험/NNG 수업/NNG 시간/NNG 때/NNG 가르치다/VV 주다/VV 개념/NNG 알다/VV 있다/VV 거의/MAG 다/MAG 적다/VV 수/NNB 있다/VV 모르다/VV 자신/NNG 생각/NNG 논리/NNG 말하다/VV 점수/NNG 주다/VV 듯/NNB 함/NNG\n",
      "최고/NNG 교수/NNG 최고/NNG 수업/NNG 과제/NNG 적/VA 않다/VV 다/MAG 말다/VV 열심히/MAG 하다/VV 다/MAG 감안하다/VV 상응하다/VV 점수/NNG 주다/VV 다/MAG 시험/NNG 생각/NNG 논리/NNG 쓰다/VV 것/NNB 중요/NNG 교과서보/NNG 생각/NNG 중시하다/VV 암튼/MAG 결론/NNG 최고/NNG 강의/NNG\n",
      "중간/NNG 받다/VV 과제/NNG 생각/NNG 잘/MAG 나오다/VV 점수/NNG 잘/MAG 나오다/VV 중간/NNG 기말/NNG 중요하다/VV 중간/NNG 중간/NNG 과제/NNG 잘/MAG 챙기다/VV 것/NNB 점수/NNG 작용하다/VV 것/NNB 같/VA 판례/NNG 평석/NNG 처음/NNG 감/NNG 잘/MAG 안/MAG 잡히다/VV 하다/VV 보다/VV 요령/NNG 생기다/VV 글발/NNG 중요하다/VV 생각하다/VV 무조건/MAG 늘어지다/VV 문장/NNG 짧/VA 요약하다/VV 형식/NNG 좋아하다/VV 시험/NNG 교재/NNG 보다/VV 필요/NNG 없/VA\n",
      "과제/NNG 매우/MAG 많/VA 리포트/NNG 장다/VV 분량/NNG 미달/NNG 내용/NNG 좋/VA 점수/NNG 잘/MAG 주심/NNG 평석/NNG 번/NNG 어려움/NNG 아직/MAG 모르다/VV 시험/NNG 책/NNG 나오다/VV 것/NNB 교수/NNG 강/NNG 잘/MAG 듣다/VV 하다/VV 판단/NNG 시험/NNG 잘/MAG 서술하다/VV 점수/NNG 잘/MAG 주심/NNG\n",
      "교재/NNG 있다/VV 딱히/MAG 시험/NNG 많이/MAG 반영/NNG 안/MAG 되다/VV 특히/MAG 기말/NNG 사례형/NNG 점수/NNG 딱히/MAG 작용/NNG 하다/VV 않다/VV 절대/NNG\n",
      "생각/NNG 학점/NNG 잘/MAG 줌/NNG 시험/NNG 잘/MAG 보다/VV 글/NNG 빨/NNG 좀/MAG 있다/VV 한중간/NNG 잘/MAG 보다/VV 기말/NNG 망치다/VV 링/NNG 훅/MAG 떨어지다/VV 조심/NNG 수업/NNG 사례/NNG 많이/MAG 들다/VV 주심/NNG 다/MAG 적다/VV 보다/VV 기본/NNG 개념/NNG 다/MAG 익히다/VV\n",
      "과제/NNG 평가/NNG 불가/NNG 받다/VV 중간/NNG 기말/NNG 잘/MAG 보다/VV 에이프/NNG 줌/NNG 수능/NNG 사/NNG 탐/NNG 법/NNG 정치하다/VV 수업/NNG 시간/NNG 잠/NNG 자다/VV 시험/NNG 보다/VV 에이프/NNG 나오다/VV 솔직히/MAG 강의/NNG 책/NNG 시험/NNG 별로/MAG 안/MAG 나오다/VV 보다/VV 되다/VV 쓰다/VV 때/NNG 없이/MAG 과제/NNG 힘주다/VV 필요/NNG 없/VA\n",
      "배우다/VV 때/NNG 하다/VV 식/NNG 주다/VV 요즘/NNG 어쩌다/VV 러/MAG\n",
      "정말/MAG 잘/MAG 가르치다/VV 주다/VV 학생/NNG 의견/NNG 하나하나/MAG 듣다/VV 주다/VV 노력/NNG 인상/NNG 깊/VA 수업/NNG 과/NNG 제도/NNG 너무/MAG 하/XSA 거의/MAG 없/VA 거/NNB 마찬가지/NNG\n",
      "것/NNB 다/MAG 좋/VA 시험/NNG 요구하다/VV 양/NNG 조금/MAG 많/VA 느낌/NNG 교수/NNG 수업/NNG 잘/MAG 하다/VV 피드백/NNG 빠르/VA 친절하다/VV 지금/NNG 보다/VV 교수/NNG 중/NNB 갓/MAG 갓/MAG 갓/MAG 시험/NNG 어렵/VA 점/NNG 점/NNG 드리다/VV 대학/NNG 수업/NNG 본질/NNG 측면/NNG 점/NNG 거/NNB 같/VA\n",
      "수업/NNG 비교하다/VV 때/NNG 가장/NNG 고퀄리티/NNG 강의/NNG 학생/NNG 의견/NNG 가장/MAG 적극/NNG 듣다/VV 하다/VV 교수/NNG 최대한/NNG 학생/NNG 편의/NNG 봐다/VV 주다/VV 하다/VV 설명/NNG 잘/MAG 하다/VV 주다/VV 강의/NNG 내용/NNG 잘/MAG 이해하다/VV 수/NNB 있다/VV 제도/NNG 다/MAG 자신/NNG 생각/NNG 적/NNB 과제/NNG 근데/MAJ 시험/NNG 번/NNB 하다/VV 보다/VV 적/NNB 없/VA 유형/NNG 문제/NNG 좀/MAG 어렵/VA 교수/NNG 시험/NNG 전/NNG 어떻/VA 작성하다/VV 하다/VV 양식/NNG 알리다/VV 주다/VV 시험/NNG 시간/NNG 안/NNG 다/MAG 작성하다/VV 내/NNG 시간/NNG 좀/MAG 부족하다/VV 느끼다/VV 지다/VV 그/MAJ 러다/VV 최대한/NNG 열심히/MAG 쓰다/VV 내다/VV 교수/NNG 학점/NNG 잘/MAG 주심/NNG 또/MAG 시험/NNG 보다/VV 전/NNG 양식/NNG 쓰다/VV 것/NNB 연습하다/VV 보다/VV 얻다/VV 것/NNB 많/VA 생각하다/VV 이번/NNG 학기/NNG 가장/MAG 만족스럽다/VV 수업/NNG\n",
      "얻다/VV 가다/VV 것/NNB 많/VA 수업/NNG 점수/NNG 학점/NNG 떠나다/VV 헌법/NNG 관하다/VV 지식/NNG 얻다/VV 생각/NNG 힘/NNG 기르다/VV 수/NNB 있다/VV 수업/NNG 좋/VA 교수/NNG 항상/MAG 학생/NNG 입장/NNG 수업/NNG 준비/NNG 하다/VV 것/NNB 느끼다/VV 지다/VV 정도/NNG 수업/NNG 준비/NNG 꼼꼼/MAG 하/XSA 설명/NNG 이해하다/VV 쉽/VA 하다/VV 주다/VV 매/NNG 학기/NNG 수업/NNG 시간/NNG 과제/NNG 수업/NNG 바라다/VV 점/NNG 제출하다/VV 하다/VV 학생/NNG 의견/NNG 최대한/NNG 반영하다/VV 수업/NNG 주다/VV 비하/NNG 이번/NNG 학기/NNG 사이버/NNG 강의하다/VV 전자/NNG 패드/NNG 필기/NNG 하다/VV 학생/NNG 최대한/NNG 쉽/VA 내용/NNG 전달하다/VV 모습/NNG 감동/NNG 받다/VV 그리고/MAJ 시험/NNG 문제/NNG 해결/NNG 능력/NNG 중점/NNG 보다/VV 문제/NNG 출제/NNG 되다/VV 최대한/NNG 열심히/MAG 공부하다/VV 만큼/NNB 작성하다/VV 좋/VA 성적/NNG 받다/VV 수/NNB 있다/VV 특히/MAG 중간/NNG 고사/NNG 경우/NNG 시험지/NNG 교수/NNG 점수/NNG 부다/VV 여/NNP 기준/NNG 평가/NNG 대하다/VV 글/NNG 쓰다/VV 메일/NNG 보내다/VV 주다/VV 기말/NNG 준비하다/VV 데/NNB 도움/NNG 되다/VV 모르다/VV 질문/NNG 메일/NNG 보내다/VV 하/XSA 설명하다/VV 주다/VV 법학부/NNG 교수/NNG 중/NNB 정말/MAG 최고입/NNG\n",
      "갓/MAG 갓/MAG 갓/MAG 그저/MAG 갓/MAG 수업/NNG 준비/NNG 시험/NNG 준비/NNG 대충/MAG 하다/VV 일/NNG 없/VA 듯/NNB 온라인/NNG 걱정/NNG 거/NNB 필요/NNG 없/VA 오프라인/NNG 수업/NNG 좋/VA 소문나다/VV 교수/NNG 온라인/NNG 되다/VV 학생/NNG 불안하다/VV 보다/VV 더/MAG 열심히/MAG 하다/VV 듯/NNB 특히/MAG 시험/NNG 문제/NNG 보다/VV 감탄하다/VV 혹시/MAG 모르다/VV 부정/NNG 행위/NNG 싹/MAG 다/MAG 감안하다/VV 시험/NNG 문제/NNG 정말/MAG 감탄하다/VV\n",
      "이번/NNG 학기/NNG 제/NNP 만족스럽다/VV 강/NNG 과제/NNG 학생/NNG 생각/NNG 들다/VV 보다/VV 하다/VV 느낌/NNG 과제/NNG 전혀/MAG 어렵/VA 않다/VV 그냥/MAG 본인/NNG 생각/NNG 적다/VV 내다/VV 되다/VV 과/NNG 제/NNG 시험/NNG 잘/MAG 보다/VV 것/NNB 중요하다/VV 전/NNG 중간/NNG 그럭저럭/MAG 보다/VV 피플/NNG 받다/VV 기/NNG 말/NNG 만회하다/VV 에이프/NNG 받다/VV 교수/NNG 학생/NNG 얘기/NNG 정말/MAG 잘/MAG 들다/VV 주다/VV 수업/NNG 내용/NNG 이해하다/VV 쉽/VA 설명/NNG 잘/MAG 하/XSV 주다/VV 정말/MAG 학생/NNG 수업/NNG 바라다/VV 점/NNG 소감/NNG 그리고/MAJ 강의/NNG 의견/NNG 세심히/MAG 챙기다/VV 보다/VV 것/NNB 같/VA 강/NNG 의견/NNG 글/NNG 조회/NNG 수/NNG 찍히다/VV 있다/VV 거/NNB 보다/VV 진심/NNG 감탄하다/VV 강/NNG 의견/NNG 진/NNG 짜/MAG 열심히/MAG 쓰다/VV 쓰다/VV 보람/NNG 있다/VV 기분/NNG\n",
      "빛/NNG 정/NNP 성적/NNG 만족스럽다/VV 나오다/VV 않다/VV 교수/NNG 하나하나/MAG 피드백/NNG 잘/MAG 하/XSV 주다/VV 학생/NNG 대하다/VV 애정/NNG 많/VA 잘/MAG 가르치다/VV 주다/VV 다/MAG 무너무/NNG 멋/NNG 있/VA 교수/NNG\n",
      "내용/NNG 법/NNG 기/NNG 초인/NNG 헌법/NNG 만큼/NNB 법학부/NNG 학생/NNG 재밌/VA 생각하다/VV 솔직히/MAG 성적/NNG 엄청/MAG 잘/MAG 나오다/VV 않다/VV 납득하다/VV 수/NNB 있다/VV 수준/NNG 받다/VV 교수/NNG 워낙/MAG 피드백/NNG 잘/MAG 받다/VV 주다/VV 노력하다/VV 좋/VA 강/NNG 듣다/VV 수/NNB 있다/VV 전공/NNG 입장/NNG 잘/MAG 모르다/VV 전공/NNG 확실히/MAG 좋/VA 기본/NNG 개념/NNG 확실히/MAG 챙기다/VV 가다/VV 수/NNB 있다/VV\n",
      "시험/NNG 좀/MAG 어렵/VA 점수/NNG 받다/VV 쉽/VA 것/NNB 시험/NNG 전/NNG 알리다/VV 주다/VV 범위/NNG 나오다/VV 과/NNG 제도/NNG 거의/MAG 없/VA 수준/NNG 교수/NNG 강의력/NNG 정말/MAG 좋/VA\n",
      "학점/NNG 잘/MAG 주다/VV 과제/NNG 버겁/VA 않다/VV 시험/NNG 유형/NNG 적응하다/VV 하다/VV 만/NNB 함/NNG 논술/NNG 목차/NNG 잘/MAG 쓰다/VV 개념/NNG 정/NNG 잘/MAG 하/XSV 주다/VV 것/NNB 중요하다/VV 시험/NNG 직전/NNG 정리하다/VV 주심/NNG\n",
      "과제/NNG 안/MAG 빡세다/VV 거/NNB 많이/MAG 내주다/VV 교수/NNG 워낙/MAG 친절하다/VV 수업/NNG 너무/MAG 재미있/VA 좋/VA 수업/NNG 시험/NNG 항상/MAG 중간/NNG 기말/NNG 다/MAG 나오다/VV 거/NNB 집다/VV 주다/VV 시험/NNG 솔직히/MAG 쉽/VA 많/VA 않다/VV 교수/NNG 얘기하다/VV 거/NNB 그대로/MAG 공부하다/VV 성적/NNG 잘/MAG 나오다/VV 중간/NNG 서술/NNG 문제/NNG 나오다/VV 기말/NNG 서술/NNG 문제/NNG 객관식/NNG 단답식/NNG 개/NNB 나오다/VV 서술/NNG 목차/NNG 정하다/VV 목차대/NNG 잘/MAG 쓰다/VV 되다/VV\n",
      "조문/NNG 위주/NNG 설명하다/VV 주다/VV 특히/MAG 학생/NNG 어려워하다/VV 포인트/NNG 정확하다/VV 집다/VV 하/XSA 알리다/VV 주다/VV 빼먹다/VV 받다/VV 그저/MAG 감사하다/VV 따름/NNB 시험/NNG 크/VA 어렵/VA 않다/VV 법학부/NNG 수업/NNG 중/NNB 학점/NNG 잘/MAG 따다/VV 좋/VA 강좌/NNG 상위/NNG 들다/VV 것/NNB 같/VA\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 30):\n",
    "    print(stemming_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'판례/NNG 평석/NNG 잘/MAG 하/XSV 성적/NNG 잘/MAG 주다/VV 것/NNB 같/VA 중간/NNG 개/NNG 망치다/VV 기말/NNG 틀리다/VV 문제/NNG 개/NNB 있다/VV 판례/NNG 평석/NNG 과제/NNG 평가/NNG 전부/NNG 성적/NNG 결국/NNG 나오다/VV'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =[]\n",
    "for doc in stemming_corpus:\n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc) #한글\n",
    "    corpus.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "review[\"corpus\"] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.to_csv(\"data/preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'판례 평석 잘 하 성적 잘 주다 것 같 중간 개 망치다 기말 틀리다 문제 개 있다 판례 평석 과제 평가 전부 성적 결국 나오다'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 파일로 저장\n",
    "with open('data/tokenized_doc.txt','w') as f:\n",
    "    for doc in corpus:\n",
    "        f.writelines(doc)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus파일 불러오기\n",
    "data =[]\n",
    "with open('data/tokenized_doc.txt','r') as f:\n",
    "    for doc in f:\n",
    "        doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "        data.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization ex.개, 겁나 -> 너무"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
