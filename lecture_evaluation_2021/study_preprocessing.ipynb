{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"study_preprocessing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c5s-rHXcHfr","executionInfo":{"status":"ok","timestamp":1620703238308,"user_tz":-540,"elapsed":16990,"user":{"displayName":"Elina Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzLYHKyx5OjO7yshsFbcda5D7a_S4v6POdl6pj=s64","userId":"01997103414232308683"}},"outputId":"3affd4ae-a08f-462a-edb6-75254d2ad5bd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xnshMU4XAJVJ"},"source":["import os\n","import sys\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","import json\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","sns.set(style='white', context='notebook', palette='deep')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2QKp6QFpAJVQ"},"source":["* basic\n","    * 가장 기초적 전처리\n","    * punctuation 제거\n","* spell check\n","    * 오탈자 교정\n","    * 줄임말 원형 복원 (i'm not happy -> I am not happy)\n","* part of speech\n","    * 형태소분석\n","    * noun, adjective, verb, adverb\n","* stemming\n","    * 형태소 분석 후 동사 원형 복원\n","* stopwords"]},{"cell_type":"code","metadata":{"id":"3BbkNiWwAJVR"},"source":["review = pd.read_csv(\"/content/drive/MyDrive/Project/창의학기제/lecture_evaluation_2021/data/review_datal_all.csv\")\n","review = pd.DataFrame(review)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"VbYP8iIrAJVR","outputId":"e681b23e-493c-43f9-a15f-ea31241f0186"},"source":["test = list(review.text)\n","len(test) #15개의 데이터"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6653"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"fxUh-91uAJVS","outputId":"46adfa71-c74f-421d-d056-5867d070607a"},"source":["\"\"\"\n","#제일 처음에 사용하던 전처리\n","#1.띄워쓰기\n","from pykospacing import spacing  \n","space_doc=[]\n","for doc in test:\n","    space_doc.append(spacing(doc))\n","#2. 맞춤법과 한글만 남기기\n","from hanspell import spell_checker\n","checked_doc =[]\n","for doc in test:\n","    check = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc) #한글\n","    check = spell_checker.check(check).checked #맞춤법\n","    checked_doc.append(check) \n","    \n","#3.stopword\n","path = 'data/stopwords-ko.txt'\n","with open(path,encoding=\"utf-8\") as f:\n","    stopwords =[word for word in f]\n","\n","#4.tokenization - 품사 (형용사, 동사에 감성분석)\n","tokenized_doc=[]\n","word_doc = []\n","tokenization = input()\n","from konlpy.tag import Okt\n","tokenizer = Okt()\n","for doc in checked_doc:\n","    if(tokenization == \"품사\"):\n","        token = [pair for pair in tokenizer.pos(doc) if pair[0] not in stopwords and len(pair[0]) > 1] \n","        words = [word for word, pos in token]\n","        tokenized_doc.append(words)\n","        word_doc.append(token)\n","    if(tokenization == \"단어\"):\n","        token = [word for word in doc.split() if word not in stopwords and len(word) > 1] \n","        tokenized_doc.append(token)\n","    if(tokenization == \"형태소\"):\n","        token = [word for word in tokenizer.morphs(doc,stem=True) if word not in stopwords and len(word) > 1] \n","        tokenized_doc.append(token)\n","print(len(tokenized_doc))\n","print(len(word_doc)) #품사일때\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#1.띄워쓰기\\nfrom pykospacing import spacing  \\nspace_doc=[]\\nfor doc in test:\\n    space_doc.append(spacing(doc))\\n#2. 맞춤법과 한글만 남기기\\nfrom hanspell import spell_checker\\nchecked_doc =[]\\nfor doc in test:\\n    check = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc) #한글\\n    check = spell_checker.check(check).checked #맞춤법\\n    checked_doc.append(check) \\n    \\n#3.stopword\\npath = \\'data/stopwords-ko.txt\\'\\nwith open(path,encoding=\"utf-8\") as f:\\n    stopwords =[word for word in f]\\n\\n#4.tokenization - 품사 (형용사, 동사에 감성분석)\\ntokenized_doc=[]\\nword_doc = []\\ntokenization = input()\\nfrom konlpy.tag import Okt\\ntokenizer = Okt()\\nfor doc in checked_doc:\\n    if(tokenization == \"품사\"):\\n        token = [pair for pair in tokenizer.pos(doc) if pair[0] not in stopwords and len(pair[0]) > 1] \\n        words = [word for word, pos in token]\\n        tokenized_doc.append(words)\\n        word_doc.append(token)\\n    if(tokenization == \"단어\"):\\n        token = [word for word in doc.split() if word not in stopwords and len(word) > 1] \\n        tokenized_doc.append(token)\\n    if(tokenization == \"형태소\"):\\n        token = [word for word in tokenizer.morphs(doc,stem=True) if word not in stopwords and len(word) > 1] \\n        tokenized_doc.append(token)\\nprint(len(tokenized_doc))\\nprint(len(word_doc)) #품사일때\\n'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"_z-h9m_nAJVT"},"source":["# basic and check spelling"]},{"cell_type":"code","metadata":{"id":"sAR5K1beAJVT"},"source":["#1. korean\n","import re\n","clean_doc =[]\n","for doc in test:\n","    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc) #한글\n","    clean_doc.append(doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nihmHizwAJVU","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1620703384776,"user_tz":-540,"elapsed":85100,"user":{"displayName":"Elina Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzLYHKyx5OjO7yshsFbcda5D7a_S4v6POdl6pj=s64","userId":"01997103414232308683"}},"outputId":"d257026e-3dce-4934-f8bd-210e6b9b025c"},"source":["'''\n","normalize-soynlp\n","띄워쓰기 pykospacing\n","맞춤법 hanspell\n","외래어 사전 다운로드\n","'''\n","#!pip install soynlp \n","#!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n","#!pip install git+https://github.com/ssut/py-hanspell.git\n","#!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" > /dev/null\n","#!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" -o confused_loanwords.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting soynlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/50/6913dc52a86a6b189419e59f9eef1b8d599cffb6f44f7bb91854165fc603/soynlp-0.0.493-py3-none-any.whl (416kB)\n","\u001b[K     |████████████████████████████████| 419kB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n","Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n","Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.0.1)\n","Installing collected packages: soynlp\n","Successfully installed soynlp-0.0.493\n","Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n","  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-nwfzmb_e\n","  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-nwfzmb_e\n","Collecting tensorflow==2.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/0a/012cc33c643d844433d13001dd1db179e7020b05ddbbd0a9dc86c38a8efa/tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7MB)\n","\u001b[K     |████████████████████████████████| 394.7MB 35kB/s \n","\u001b[?25hRequirement already satisfied: keras>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.4) (2.4.3)\n","Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.4) (2.10.0)\n","Collecting argparse>=1.4.0\n","  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.6.3)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.2)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.3.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.15.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.3.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.2.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.12.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.32.0)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.36.2)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12.1)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.7.4.3)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.12.4)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.19.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.4.3->pykospacing==0.4) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.4.3->pykospacing==0.4) (3.13)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.3.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (56.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.28.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.10.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.24.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.2.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.8)\n","Building wheels for collected packages: pykospacing\n","  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pykospacing: filename=pykospacing-0.4-cp37-none-any.whl size=2255638 sha256=e71f5b6df8127535058bbc39dff84dd93731267483e33d7d9a4453e14f2dd4f8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-olksoi8p/wheels/4d/45/58/e26cb2b7f6a063d234158c6fd1e5700f6e15b99d67154340ba\n","Successfully built pykospacing\n","Installing collected packages: tensorflow, argparse, pykospacing\n","  Found existing installation: tensorflow 2.4.1\n","    Uninstalling tensorflow-2.4.1:\n","      Successfully uninstalled tensorflow-2.4.1\n","Successfully installed argparse-1.4.0 pykospacing-0.4 tensorflow-2.4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["argparse"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting git+https://github.com/ssut/py-hanspell.git\n","  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-mwkra1dp\n","  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-mwkra1dp\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n","Building wheels for collected packages: py-hanspell\n","  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp37-none-any.whl size=4854 sha256=99ec86bbb5a79af5bec3d0ac14471c41ea28e6f83f4a9ce79395a88697f89ce8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-pzbqc8yx/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n","Successfully built py-hanspell\n","Installing collected packages: py-hanspell\n","Successfully installed py-hanspell-1.1\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100   408    0   408    0     0    969      0 --:--:-- --:--:-- --:--:--   969\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 19779  100 19779    0     0  30382      0 --:--:-- --:--:-- --:--:-- 30382\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zBxT9bUGAJVU"},"source":["lownword_map = {}\n","lownword_data = open('confused_loanwords.txt', 'r', encoding='utf-8')\n","\n","lines = lownword_data.readlines()\n","\n","for line in lines:\n","    line = line.strip()\n","    miss_spell = line.split('\\t')[0]\n","    ori_word = line.split('\\t')[1]\n","    lownword_map[miss_spell] = ori_word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SJf_0lXAJVV","outputId":"dccfac28-ce3e-40f9-9ea1-c6c0294768ea"},"source":["pip install numpy==1.19.2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting numpy==1.19.2\n","  Downloading numpy-1.19.2-cp38-cp38-macosx_10_9_x86_64.whl (15.3 MB)\n","\u001b[K     |████████████████████████████████| 15.3 MB 415 kB/s eta 0:00:01\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.0\n","    Uninstalling numpy-1.19.0:\n","      Successfully uninstalled numpy-1.19.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n","Successfully installed numpy-1.19.2\n","Note: you may need to restart the kernel to use updated packages.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QErae1SnAJVV"},"source":["from pykospacing import spacing\n","from hanspell import spell_checker\n","from soynlp.normalizer import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DyPlepcVAJVV"},"source":["spell_preprocessed_doc = []\n","for doc in clean_doc:\n","    spaced_doc = spacing(doc) #띄워쓰기\n","    spelled_doc = spell_checker.check(spaced_doc) #맞춤법\n","    checked_doc = spelled_doc.checked\n","    normalized_doc = repeat_normalize(checked_doc) #정규화\n","    for lownword in lownword_map:\n","        normalized_doc = normalized_doc.replace(lownword, lownword_map[lownword])\n","    spell_preprocessed_doc.append(normalized_doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UB3Tn7DHAJVW"},"source":["review[\"spell_preprocessed\"] = spell_preprocessed_doc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9BkaPmsAJVW"},"source":["# tokenizatioin and stemming\n","- mecab (stemming까지 추가해서 비교해볼 예정)\n","- khaiii"]},{"cell_type":"code","metadata":{"id":"IqGIgd4wAJVW"},"source":["#Mecab \n","#3.stopword\n","path = 'data/stopwords-ko.txt'\n","with open(path,encoding=\"utf-8\") as f:\n","    stopwords =[word for word in f]\n","\n","#4.tokenization - 품사 (형용사, 동사에 감성분석)\n","tokenized_doc=[]\n","word_doc = []\n","from konlpy.tag import Mecab\n","tokenizer = Mecab()\n","for doc in spell_preprocessed_doc:\n","    token = [pair for pair in tokenizer.pos(doc) if pair[0] not in stopwords and len(pair[0]) > 1] \n","    words = [word for word, pos in token]\n","    tokenized_doc.append(words)\n","    word_doc.append(token)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fS6F63HLAJVW"},"source":["#khaiii\n","from khaiii import KhaiiiApi\n","api = KhaiiiApi()\n","\n","significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n","\n","def pos_text(texts):\n","    corpus = []\n","    error = []\n","    for sent in texts:\n","        pos_tagged = ''\n","        try:\n","            words = api.analyze(sent)\n","            for word in words:\n","                for morph in word.morphs:\n","                    if morph.tag in significant_tags:\n","                        pos_tagged += morph.lex + '/' + morph.tag + ' '\n","            corpus.append(pos_tagged.strip())\n","        except:\n","                error.append(sent)\n","                corpus.append(\"\")\n","    return corpus, error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9a0zRMp9AJVX","outputId":"a14867bf-4590-4b7b-cefa-a1871502c3dc"},"source":["pos_tagged_corpus, error = pos_text(spell_preprocessed_doc)\n","print(len(pos_tagged_corpus),len(error)) #error는 수작업..?"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6653 102\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tCIi8MzUAJVX"},"source":["동사를 원형으로 복원하도록 하겠습니다.\n","규칙은 다음과 같습니다.\n","\n","1. NNG|NNP|NNB + XSV|XSA --> NNG|NNP|NNB + XSV|XSA + 다\n","2. NNG|NNP|NNB + XSA + VX --> NNG|NNP + XSA + 다\n","3. VV --> VV + 다\n","4. VX --> VX + 다"]},{"cell_type":"code","metadata":{"id":"QjVIPJipAJVY"},"source":["p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n","p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n","p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n","p4 = re.compile('[가-힣A-Za-z0-9]+/VX')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xY0luiGLAJVY"},"source":["def stemming_text(text):\n","    corpus = []\n","    for sent in text:\n","        ori_sent = sent\n","        mached_terms = re.findall(p1, ori_sent)\n","        for terms in mached_terms:\n","            ori_terms = terms\n","            modi_terms = ''\n","            for term in terms.split(' '):\n","                lemma = term.split('/')[0]\n","                tag = term.split('/')[-1]\n","                modi_terms += lemma\n","            modi_terms += '다/VV'\n","            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n","        \n","        mached_terms = re.findall(p2, ori_sent)\n","        for terms in mached_terms:\n","            ori_terms = terms\n","            modi_terms = ''\n","            for term in terms.split(' '):\n","                lemma = term.split('/')[0]\n","                tag = term.split('/')[-1]\n","                if tag != 'VX':\n","                    modi_terms += lemma\n","            modi_terms += '다/VV'\n","            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n","\n","        mached_terms = re.findall(p3, ori_sent)\n","        for terms in mached_terms:\n","            ori_terms = terms\n","            modi_terms = ''\n","            for term in terms.split(' '):\n","                lemma = term.split('/')[0]\n","                tag = term.split('/')[-1]\n","                modi_terms += lemma\n","            if '다' != modi_terms[-1]:\n","                modi_terms += '다'\n","            modi_terms += '/VV'\n","            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n","\n","        mached_terms = re.findall(p4, ori_sent)\n","        for terms in mached_terms:\n","            ori_terms = terms\n","            modi_terms = ''\n","            for term in terms.split(' '):\n","                lemma = term.split('/')[0]\n","                tag = term.split('/')[-1]\n","                modi_terms += lemma\n","            if '다' != modi_terms[-1]:\n","                modi_terms += '다'\n","            modi_terms += '/VV'\n","            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n","        corpus.append(ori_sent)\n","    return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OxSGO3EAJVY"},"source":["stemming_corpus = stemming_text(pos_tagged_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80EPKVTjAJVZ","outputId":"d088c0ab-b591-463f-b6ff-c368a507a7a9"},"source":["for i in range(0, 30):\n","    print(stemming_corpus[i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["판례/NNG 평석/NNG 잘/MAG 하/XSV 성적/NNG 잘/MAG 주다/VV 것/NNB 같/VA 중간/NNG 개/NNG 망치다/VV 기말/NNG 틀리다/VV 문제/NNG 개/NNB 있다/VV 판례/NNG 평석/NNG 과제/NNG 평가/NNG 전부/NNG 성적/NNG 결국/NNG 나오다/VV\n","과제/NNG 되다/VV 귀찮/VA 많/VA 시험/NNG 말발/NNG 좋/VA 되다/VV 시험/NNG 망치다/VV 과제/NNG 열심히/MAG 하다/VV 점수/NNG 잘/MAG 받다/VV\n","수업/NNG 띄우다/VV 놓다/VV 하다/VV 보통/NNG 교안/NNG 없/VA 관련/NNG 사례/NNG 중심/NNG 많이/MAG 얘기하다/VV 심/NNG 수업/NNG 잘/MAG 듣다/VV 시험/NNG 잘/MAG 보다/VV 수/NNB 있다/VV 성적/NNG 후하다/VV 주다/VV 편/NNB 다만/MAG 과제/NNG 분량/NNG 채우다/VV 쓰다/VV 쉽/VA 않다/VV\n","교재/NNG 없/VA 내용/NNG 많이/MAG 얘기하다/VV 주다/VV 말씀하다/VV 내용/NNG 시험/NNG 문제/NNG 내다/VV 교재/NNG 그렇/VA 크/VA 필요/NNG 없/VA 과제/NNG 좀/MAG 시간/NNG 오래/MAG 걸리다/VV 하다/VV 데/NNG 판례/NNG 평석/NNG 민사/NNG 소송법/NNG 강의/NNG 내주다/VV 어차피/MAG 내년/NNG 하다/VV 되다/VV 계속하다/VV 되다/VV 거/NNB 연습하다/VV 다/MAG 셈/NNB 치다/VV 해/NNG 보다/VV 길/NNG 점수/NNG 나/NNB 름/NNG 잘/MAG 주다/VV 것/NNB 같/VA 방청/NNG 재미있/VA 상당히/MAG 만족하다/VV 강의/NNG 강의/NNG 내용/NNG 재밌/VA 시험/NNG 문제/NNG 괜찮/VA 학점/NNG 원래/NNG 잘/MAG 주다/VV 교수/NNG 법학/NNG 과/NNG 들어오다/VV 강의/NNG 듣다/VV 전/NNG 수강하다/VV 최적되다/VV 과목/NNG 듯/NNB\n","사이버/NNG 강의라/NNG 괜찮/VA 과제/NNG 꽤/MAG 많/VA 시험/NNG 수업/NNG 잘/MAG 듣다/VV 잘/MAG 나오다/VV\n","과제/NNG 힘/VA 듦/NNG 수업/NNG 집중하다/VV 재미/NNG 있다/VV 매우/MAG 졸다/VV 림/NNG\n","교수/NNG 진짜/NNG 좋/VA 재밌/VA 수업하다/VV 노력하다/VV 이름/NNG 외/NNB 우다/VV 노력하다/VV 것/NNB 눈/NNG 보이다/VV 다만/MAG 재미/NNG 없/VA 시/NNG 종/NNG 치다/VV 때쯤/NNG 되다/VV 앞/NNG 애/NNG 다/MAG 졸다/VV 있다/VV 평석/NNG 방청/NNG 같/VA 과/NNG 있다/VV 글/NNG 쓰다/VV 양/NNG 늘리다/VV 잘/MAG 쓰다/VV 사람/NNG 문제없/VA 성/NNG 만/NNB 있다/VV 이쁘/VA 주다/VV 것/NNB 같/VA 수업/NNG 시간/NNG 말하다/VV 사례/NNG 위주/NNG 공부하다/VV 성적/NNG 나쁘/VA 않다/VV 나오다/VV\n","교재/NNG 정말/MAG 필요/NNG 없/VA 당연히/MAG 시험/NNG 문제/NNG 그다지/MAG 영향/NNG 안/MAG 줌/NNG 교수/NNG 수업/NNG 전/NNG 따로/MAG 호명하다/VV 눈/NNG 마주치다/VV 심/NNG 수업/NNG 내용/NNG 좋/VA 과제/NNG 많이/MAG 빡/MAG 셈/NNG 내용/NNG 빡/MAG 세/NNG 분량/NNG 채우다/VV 힘들/VA 것/NNB 큼/NNB\n","일단/MAG 법학부/NNG 수업/NNG 중/NNB 재미있/VA 그리고/MAJ 열공하다/VV 성적/NNG 또한/MAG 뒤/NNG 따/NNB 르/VA 분/MAG 하/XSA\n","교재/NNG 꼼꼼히/MAG 안/MAG 읽다/VV 교수/NNG 강/NNG 녹음하다/VV 놓다/VV 시험/NNG 기간/NNG 전/NNG 번/NNB 듣다/VV 보다/VV 하다/VV 시험/NNG 어렵/VA 않다/VV 풀다/VV 수/NNB 있다/VV 문제/NNG 대부분/NNG 직접/MAG 평결/NNG 내리다/VV 형식임/NNG 학기/NNG 과제/NNG 번/NNB 다/MAG 장/NNG 최소/NNG 분량/NNG 그런데/MAJ 분량/NNG 안/MAG 채우다/VV 내용/NNG 애쓰다/VV 티/NNG 나다/VV 점수/NNG 잘/MAG 주다/VV 피드백/NNG 있다/VV 좋/VA\n","제대로/MAG 들다/VV 보다/VV 배우다/VV 것/NNB 많/VA 재밌/VA 강의/NNG 그러나/MAJ 제대로/MAG 듣다/VV 힘들/VA 강/NNG 매우/MAG 졸림/NNG 과제/NNG 번/NNB 있다/VV 과제하다/VV 하다/VV 수/NNB 있다/VV 과제/NNG 아/NNG 글/NNG 잘/MAG 쓰다/VV 거/NNB 좋아다/VV 하/XSA 그러다/VV 열심히/MAG 하다/VV 이상/NNG 주/NNG 심/NNG 시험/NNG 수업/NNG 시간/NNG 때/NNG 가르치다/VV 주다/VV 개념/NNG 알다/VV 있다/VV 거의/MAG 다/MAG 적다/VV 수/NNB 있다/VV 모르다/VV 자신/NNG 생각/NNG 논리/NNG 말하다/VV 점수/NNG 주다/VV 듯/NNB 함/NNG\n","최고/NNG 교수/NNG 최고/NNG 수업/NNG 과제/NNG 적/VA 않다/VV 다/MAG 말다/VV 열심히/MAG 하다/VV 다/MAG 감안하다/VV 상응하다/VV 점수/NNG 주다/VV 다/MAG 시험/NNG 생각/NNG 논리/NNG 쓰다/VV 것/NNB 중요/NNG 교과서보/NNG 생각/NNG 중시하다/VV 암튼/MAG 결론/NNG 최고/NNG 강의/NNG\n","중간/NNG 받다/VV 과제/NNG 생각/NNG 잘/MAG 나오다/VV 점수/NNG 잘/MAG 나오다/VV 중간/NNG 기말/NNG 중요하다/VV 중간/NNG 중간/NNG 과제/NNG 잘/MAG 챙기다/VV 것/NNB 점수/NNG 작용하다/VV 것/NNB 같/VA 판례/NNG 평석/NNG 처음/NNG 감/NNG 잘/MAG 안/MAG 잡히다/VV 하다/VV 보다/VV 요령/NNG 생기다/VV 글발/NNG 중요하다/VV 생각하다/VV 무조건/MAG 늘어지다/VV 문장/NNG 짧/VA 요약하다/VV 형식/NNG 좋아하다/VV 시험/NNG 교재/NNG 보다/VV 필요/NNG 없/VA\n","과제/NNG 매우/MAG 많/VA 리포트/NNG 장다/VV 분량/NNG 미달/NNG 내용/NNG 좋/VA 점수/NNG 잘/MAG 주심/NNG 평석/NNG 번/NNG 어려움/NNG 아직/MAG 모르다/VV 시험/NNG 책/NNG 나오다/VV 것/NNB 교수/NNG 강/NNG 잘/MAG 듣다/VV 하다/VV 판단/NNG 시험/NNG 잘/MAG 서술하다/VV 점수/NNG 잘/MAG 주심/NNG\n","교재/NNG 있다/VV 딱히/MAG 시험/NNG 많이/MAG 반영/NNG 안/MAG 되다/VV 특히/MAG 기말/NNG 사례형/NNG 점수/NNG 딱히/MAG 작용/NNG 하다/VV 않다/VV 절대/NNG\n","생각/NNG 학점/NNG 잘/MAG 줌/NNG 시험/NNG 잘/MAG 보다/VV 글/NNG 빨/NNG 좀/MAG 있다/VV 한중간/NNG 잘/MAG 보다/VV 기말/NNG 망치다/VV 링/NNG 훅/MAG 떨어지다/VV 조심/NNG 수업/NNG 사례/NNG 많이/MAG 들다/VV 주심/NNG 다/MAG 적다/VV 보다/VV 기본/NNG 개념/NNG 다/MAG 익히다/VV\n","과제/NNG 평가/NNG 불가/NNG 받다/VV 중간/NNG 기말/NNG 잘/MAG 보다/VV 에이프/NNG 줌/NNG 수능/NNG 사/NNG 탐/NNG 법/NNG 정치하다/VV 수업/NNG 시간/NNG 잠/NNG 자다/VV 시험/NNG 보다/VV 에이프/NNG 나오다/VV 솔직히/MAG 강의/NNG 책/NNG 시험/NNG 별로/MAG 안/MAG 나오다/VV 보다/VV 되다/VV 쓰다/VV 때/NNG 없이/MAG 과제/NNG 힘주다/VV 필요/NNG 없/VA\n","배우다/VV 때/NNG 하다/VV 식/NNG 주다/VV 요즘/NNG 어쩌다/VV 러/MAG\n","정말/MAG 잘/MAG 가르치다/VV 주다/VV 학생/NNG 의견/NNG 하나하나/MAG 듣다/VV 주다/VV 노력/NNG 인상/NNG 깊/VA 수업/NNG 과/NNG 제도/NNG 너무/MAG 하/XSA 거의/MAG 없/VA 거/NNB 마찬가지/NNG\n","것/NNB 다/MAG 좋/VA 시험/NNG 요구하다/VV 양/NNG 조금/MAG 많/VA 느낌/NNG 교수/NNG 수업/NNG 잘/MAG 하다/VV 피드백/NNG 빠르/VA 친절하다/VV 지금/NNG 보다/VV 교수/NNG 중/NNB 갓/MAG 갓/MAG 갓/MAG 시험/NNG 어렵/VA 점/NNG 점/NNG 드리다/VV 대학/NNG 수업/NNG 본질/NNG 측면/NNG 점/NNG 거/NNB 같/VA\n","수업/NNG 비교하다/VV 때/NNG 가장/NNG 고퀄리티/NNG 강의/NNG 학생/NNG 의견/NNG 가장/MAG 적극/NNG 듣다/VV 하다/VV 교수/NNG 최대한/NNG 학생/NNG 편의/NNG 봐다/VV 주다/VV 하다/VV 설명/NNG 잘/MAG 하다/VV 주다/VV 강의/NNG 내용/NNG 잘/MAG 이해하다/VV 수/NNB 있다/VV 제도/NNG 다/MAG 자신/NNG 생각/NNG 적/NNB 과제/NNG 근데/MAJ 시험/NNG 번/NNB 하다/VV 보다/VV 적/NNB 없/VA 유형/NNG 문제/NNG 좀/MAG 어렵/VA 교수/NNG 시험/NNG 전/NNG 어떻/VA 작성하다/VV 하다/VV 양식/NNG 알리다/VV 주다/VV 시험/NNG 시간/NNG 안/NNG 다/MAG 작성하다/VV 내/NNG 시간/NNG 좀/MAG 부족하다/VV 느끼다/VV 지다/VV 그/MAJ 러다/VV 최대한/NNG 열심히/MAG 쓰다/VV 내다/VV 교수/NNG 학점/NNG 잘/MAG 주심/NNG 또/MAG 시험/NNG 보다/VV 전/NNG 양식/NNG 쓰다/VV 것/NNB 연습하다/VV 보다/VV 얻다/VV 것/NNB 많/VA 생각하다/VV 이번/NNG 학기/NNG 가장/MAG 만족스럽다/VV 수업/NNG\n","얻다/VV 가다/VV 것/NNB 많/VA 수업/NNG 점수/NNG 학점/NNG 떠나다/VV 헌법/NNG 관하다/VV 지식/NNG 얻다/VV 생각/NNG 힘/NNG 기르다/VV 수/NNB 있다/VV 수업/NNG 좋/VA 교수/NNG 항상/MAG 학생/NNG 입장/NNG 수업/NNG 준비/NNG 하다/VV 것/NNB 느끼다/VV 지다/VV 정도/NNG 수업/NNG 준비/NNG 꼼꼼/MAG 하/XSA 설명/NNG 이해하다/VV 쉽/VA 하다/VV 주다/VV 매/NNG 학기/NNG 수업/NNG 시간/NNG 과제/NNG 수업/NNG 바라다/VV 점/NNG 제출하다/VV 하다/VV 학생/NNG 의견/NNG 최대한/NNG 반영하다/VV 수업/NNG 주다/VV 비하/NNG 이번/NNG 학기/NNG 사이버/NNG 강의하다/VV 전자/NNG 패드/NNG 필기/NNG 하다/VV 학생/NNG 최대한/NNG 쉽/VA 내용/NNG 전달하다/VV 모습/NNG 감동/NNG 받다/VV 그리고/MAJ 시험/NNG 문제/NNG 해결/NNG 능력/NNG 중점/NNG 보다/VV 문제/NNG 출제/NNG 되다/VV 최대한/NNG 열심히/MAG 공부하다/VV 만큼/NNB 작성하다/VV 좋/VA 성적/NNG 받다/VV 수/NNB 있다/VV 특히/MAG 중간/NNG 고사/NNG 경우/NNG 시험지/NNG 교수/NNG 점수/NNG 부다/VV 여/NNP 기준/NNG 평가/NNG 대하다/VV 글/NNG 쓰다/VV 메일/NNG 보내다/VV 주다/VV 기말/NNG 준비하다/VV 데/NNB 도움/NNG 되다/VV 모르다/VV 질문/NNG 메일/NNG 보내다/VV 하/XSA 설명하다/VV 주다/VV 법학부/NNG 교수/NNG 중/NNB 정말/MAG 최고입/NNG\n","갓/MAG 갓/MAG 갓/MAG 그저/MAG 갓/MAG 수업/NNG 준비/NNG 시험/NNG 준비/NNG 대충/MAG 하다/VV 일/NNG 없/VA 듯/NNB 온라인/NNG 걱정/NNG 거/NNB 필요/NNG 없/VA 오프라인/NNG 수업/NNG 좋/VA 소문나다/VV 교수/NNG 온라인/NNG 되다/VV 학생/NNG 불안하다/VV 보다/VV 더/MAG 열심히/MAG 하다/VV 듯/NNB 특히/MAG 시험/NNG 문제/NNG 보다/VV 감탄하다/VV 혹시/MAG 모르다/VV 부정/NNG 행위/NNG 싹/MAG 다/MAG 감안하다/VV 시험/NNG 문제/NNG 정말/MAG 감탄하다/VV\n","이번/NNG 학기/NNG 제/NNP 만족스럽다/VV 강/NNG 과제/NNG 학생/NNG 생각/NNG 들다/VV 보다/VV 하다/VV 느낌/NNG 과제/NNG 전혀/MAG 어렵/VA 않다/VV 그냥/MAG 본인/NNG 생각/NNG 적다/VV 내다/VV 되다/VV 과/NNG 제/NNG 시험/NNG 잘/MAG 보다/VV 것/NNB 중요하다/VV 전/NNG 중간/NNG 그럭저럭/MAG 보다/VV 피플/NNG 받다/VV 기/NNG 말/NNG 만회하다/VV 에이프/NNG 받다/VV 교수/NNG 학생/NNG 얘기/NNG 정말/MAG 잘/MAG 들다/VV 주다/VV 수업/NNG 내용/NNG 이해하다/VV 쉽/VA 설명/NNG 잘/MAG 하/XSV 주다/VV 정말/MAG 학생/NNG 수업/NNG 바라다/VV 점/NNG 소감/NNG 그리고/MAJ 강의/NNG 의견/NNG 세심히/MAG 챙기다/VV 보다/VV 것/NNB 같/VA 강/NNG 의견/NNG 글/NNG 조회/NNG 수/NNG 찍히다/VV 있다/VV 거/NNB 보다/VV 진심/NNG 감탄하다/VV 강/NNG 의견/NNG 진/NNG 짜/MAG 열심히/MAG 쓰다/VV 쓰다/VV 보람/NNG 있다/VV 기분/NNG\n","빛/NNG 정/NNP 성적/NNG 만족스럽다/VV 나오다/VV 않다/VV 교수/NNG 하나하나/MAG 피드백/NNG 잘/MAG 하/XSV 주다/VV 학생/NNG 대하다/VV 애정/NNG 많/VA 잘/MAG 가르치다/VV 주다/VV 다/MAG 무너무/NNG 멋/NNG 있/VA 교수/NNG\n","내용/NNG 법/NNG 기/NNG 초인/NNG 헌법/NNG 만큼/NNB 법학부/NNG 학생/NNG 재밌/VA 생각하다/VV 솔직히/MAG 성적/NNG 엄청/MAG 잘/MAG 나오다/VV 않다/VV 납득하다/VV 수/NNB 있다/VV 수준/NNG 받다/VV 교수/NNG 워낙/MAG 피드백/NNG 잘/MAG 받다/VV 주다/VV 노력하다/VV 좋/VA 강/NNG 듣다/VV 수/NNB 있다/VV 전공/NNG 입장/NNG 잘/MAG 모르다/VV 전공/NNG 확실히/MAG 좋/VA 기본/NNG 개념/NNG 확실히/MAG 챙기다/VV 가다/VV 수/NNB 있다/VV\n","시험/NNG 좀/MAG 어렵/VA 점수/NNG 받다/VV 쉽/VA 것/NNB 시험/NNG 전/NNG 알리다/VV 주다/VV 범위/NNG 나오다/VV 과/NNG 제도/NNG 거의/MAG 없/VA 수준/NNG 교수/NNG 강의력/NNG 정말/MAG 좋/VA\n","학점/NNG 잘/MAG 주다/VV 과제/NNG 버겁/VA 않다/VV 시험/NNG 유형/NNG 적응하다/VV 하다/VV 만/NNB 함/NNG 논술/NNG 목차/NNG 잘/MAG 쓰다/VV 개념/NNG 정/NNG 잘/MAG 하/XSV 주다/VV 것/NNB 중요하다/VV 시험/NNG 직전/NNG 정리하다/VV 주심/NNG\n","과제/NNG 안/MAG 빡세다/VV 거/NNB 많이/MAG 내주다/VV 교수/NNG 워낙/MAG 친절하다/VV 수업/NNG 너무/MAG 재미있/VA 좋/VA 수업/NNG 시험/NNG 항상/MAG 중간/NNG 기말/NNG 다/MAG 나오다/VV 거/NNB 집다/VV 주다/VV 시험/NNG 솔직히/MAG 쉽/VA 많/VA 않다/VV 교수/NNG 얘기하다/VV 거/NNB 그대로/MAG 공부하다/VV 성적/NNG 잘/MAG 나오다/VV 중간/NNG 서술/NNG 문제/NNG 나오다/VV 기말/NNG 서술/NNG 문제/NNG 객관식/NNG 단답식/NNG 개/NNB 나오다/VV 서술/NNG 목차/NNG 정하다/VV 목차대/NNG 잘/MAG 쓰다/VV 되다/VV\n","조문/NNG 위주/NNG 설명하다/VV 주다/VV 특히/MAG 학생/NNG 어려워하다/VV 포인트/NNG 정확하다/VV 집다/VV 하/XSA 알리다/VV 주다/VV 빼먹다/VV 받다/VV 그저/MAG 감사하다/VV 따름/NNB 시험/NNG 크/VA 어렵/VA 않다/VV 법학부/NNG 수업/NNG 중/NNB 학점/NNG 잘/MAG 따다/VV 좋/VA 강좌/NNG 상위/NNG 들다/VV 것/NNB 같/VA\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"SamEVq2FAJVZ","outputId":"cd65534b-7529-47df-8d1f-891342a617f7"},"source":["stemming_corpus[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'판례/NNG 평석/NNG 잘/MAG 하/XSV 성적/NNG 잘/MAG 주다/VV 것/NNB 같/VA 중간/NNG 개/NNG 망치다/VV 기말/NNG 틀리다/VV 문제/NNG 개/NNB 있다/VV 판례/NNG 평석/NNG 과제/NNG 평가/NNG 전부/NNG 성적/NNG 결국/NNG 나오다/VV'"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"344MZweSAJVZ"},"source":["corpus =[]\n","for doc in stemming_corpus:\n","    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc) #한글\n","    corpus.append(doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoN3oQd3AJVZ"},"source":["review[\"corpus\"] = corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUGe2gk7AJVa"},"source":["review.to_csv(\"data/preprocessing.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"M45uv-poAJVa","outputId":"4596e091-c30e-4fa9-8764-810026a48ccb"},"source":["corpus[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'판례 평석 잘 하 성적 잘 주다 것 같 중간 개 망치다 기말 틀리다 문제 개 있다 판례 평석 과제 평가 전부 성적 결국 나오다'"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"SlBih-NVAJVa"},"source":["#전처리 파일로 저장\n","with open('data/tokenized_doc.txt','w') as f:\n","    for doc in corpus:\n","        f.writelines(doc)\n","        f.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkftVZ3CAJVc"},"source":["#corpus파일 불러오기\n","data =[]\n","with open('data/tokenized_doc.txt','r') as f:\n","    for doc in f:\n","        doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n","        data.append(doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzkEIZmdAJVc"},"source":["#lemmatization ex.개, 겁나 -> 너무"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZArJ1L3R72lI","executionInfo":{"status":"ok","timestamp":1620712018152,"user_tz":-540,"elapsed":766,"user":{"displayName":"Elina Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzLYHKyx5OjO7yshsFbcda5D7a_S4v6POdl6pj=s64","userId":"01997103414232308683"}},"outputId":"b931766e-86e1-4482-8952-208b430d453f"},"source":["text = \"안녕하세요 강의입니다.\" \n","\"강의\" in text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"7jrXALXw9n7W"},"source":[""],"execution_count":null,"outputs":[]}]}